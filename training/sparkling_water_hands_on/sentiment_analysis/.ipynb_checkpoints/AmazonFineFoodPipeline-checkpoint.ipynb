{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkling Water Pipeline Productionalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Sparkling Water provides access to H2O algorithms and publishes API to integrate them as part of regular Spark pipelines. This feature allows for seamless training and deployment of H2O algorithms in the Spark environment. Furthermore, trained pipelines do not require H2O runtime anymore (thanks to MOJO representation of trained H2O models) which enables variety of deployment scenarios. Sparkling Water can be also used for deplying Driverless AI models.\n",
    "\n",
    "Moreover, by supporting Python and Scala environment, we enable a simple transfer of modeling results between data scientists (Python land) and production (JVM land).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "The goal of this hands-on is to:\n",
    "  - show integration of H2O models into Spark pipelines using PySpark and PySparkling\n",
    "  - demonstrate deployment of the trained pipeline in the context of JVM and Spark streaming\n",
    "  \n",
    "Our modeling goal is to predict sentiment of Amazon food reviews. For this purpose, we use a pre-processed dataset from [SNAP repository](https://snap.stanford.edu/data/web-FineFoods.html). The dataset contains multiple columns but for simplicity, we will use only date, summary and overall score. The score helps us to approximate sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scenario](./img/scenario.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's verify that `SparkSession` is available in the notebook environment. We do not need to explicitly create `SparkSession` as it is created for us\n",
    "automatically during start of the Jupyter notebook. This works because of the Jupyter is set up with a Spark kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jakubs-mbp.0xdata.loc:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10fb3d940>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare `H2OContext`\n",
    "\n",
    "We will start `H2OContext` in the so called _internal backend_ mode. The means H2O is sharing JVM with Spark (see details in [Sparkling Water documentation](https://github.com/h2oai/sparkling-water/blob/rel-2.2/doc/tutorials/backends.rst)).\n",
    "\n",
    "The following call initializes H2O on each Spark executor in the Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkling Water Context:\n",
      " * H2O name: sparkling-water-kuba_local-1549059517922\n",
      " * cluster size: 1\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (driver,jakubs-mbp.0xdata.loc,54321)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://172.16.2.58:54321 (CMD + click in Mac OSX)\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from pysparkling import *\n",
    "hc = H2OContext.getOrCreate(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: the reported IP is a private IP of docker container where the demo is running.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use H2O to load data using H2O since it does pretty good job to guess all nuances of input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "import h2o\n",
    "reviews_h2o = h2o.upload_file(\"/Users/kuba/Downloads/AmazonReviews_Train.csv\", \"reviews.hex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data table in H2O flow\n",
    "\n",
    "At this point, we can explore data directly in this notebook, or we can access H2O Flow and explore data and its properties directly there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert H2O frame to Spark frame se we can pass it as the input to the pipeline\n",
    "\n",
    "After data exploration, we can start with data munging. We are going to use Spark, hence we need to publish H2O frame as Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_spark = hc.as_spark_frame(reviews_h2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick #1: Save the original Spark schema\n",
    "\n",
    "At this point we will save the schema of input data and we will reuse it later to configure deployed Spark streaming application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = false)\n",
      " |-- ProductId: string (nullable = false)\n",
      " |-- UserId: string (nullable = false)\n",
      " |-- ProfileName: string (nullable = false)\n",
      " |-- HelpfulnessNumerator: short (nullable = false)\n",
      " |-- HelpfulnessDenominator: short (nullable = false)\n",
      " |-- Score: byte (nullable = false)\n",
      " |-- Time: integer (nullable = false)\n",
      " |-- Summary: string (nullable = false)\n",
      " |-- Text: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_spark.printSchema()\n",
    "\n",
    "with open('schema.json','w') as f:\n",
    "    f.write(str(reviews_spark.schema.json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's define all the stages for the pipeline\n",
    "\n",
    "The Spark pipelines are composed of various transformers. In our example, we combine a few Spark transformers to clean up textual data and transform it into numerical format. The pipeline is finalized by training an H2O XGBoost binomial model.\n",
    "\n",
    "> Note: The pipeline stages are not executed right away, they are executed during each fit and transform call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transformer to drop unnecessary columns\n",
    "The Spark `SQLTransformer` allows for using SQL to munge data.\n",
    "\n",
    "As part of this transformer, we convert timestamp to the human readable date string:\n",
    "\n",
    "We are selecting just `Score`, `Time` and `Summary` columns. The goal of this demo is to predict sentiment, ie, whether the review is positive or negative. The review can be influenced by several aspects. The `Summary` is of course the mostly important information, but `Time` can influence the model as well. For example, people may tend to give higher reviews on Friday evenings because there's a weekend in from of them :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "colSelect = SQLTransformer(\n",
    "    statement=\"SELECT Score, from_unixtime(Time) as Time, Summary FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick #2: Explore intermediate results\n",
    "To explore intermediate results, we can also invoke defined transformer directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+--------------------+\n",
      "|Score|               Time|             Summary|\n",
      "+-----+-------------------+--------------------+\n",
      "|    5|2011-04-26 17:00:00|Good Quality Dog ...|\n",
      "|    1|2012-09-06 17:00:00|   Not as Advertised|\n",
      "|    4|2008-08-17 17:00:00|\"Delight\" says it...|\n",
      "|    2|2011-06-12 17:00:00|      Cough Medicine|\n",
      "|    5|2012-10-20 17:00:00|         Great taffy|\n",
      "|    4|2012-07-11 17:00:00|          Nice Taffy|\n",
      "|    5|2012-06-19 17:00:00|Great!  Just as g...|\n",
      "|    5|2012-05-02 17:00:00|Wonderful, tasty ...|\n",
      "|    5|2011-11-22 16:00:00|          Yay Barley|\n",
      "|    5|2012-10-25 17:00:00|    Healthy Dog Food|\n",
      "|    5|2005-02-07 16:00:00|The Best Hot Sauc...|\n",
      "|    5|2010-08-26 17:00:00|My cats LOVE this...|\n",
      "|    1|2012-06-12 17:00:00|My Cats Are Not F...|\n",
      "|    4|2010-11-04 17:00:00|   fresh and greasy!|\n",
      "|    5|2010-03-11 16:00:00|Strawberry Twizzl...|\n",
      "|    5|2009-12-28 16:00:00|Lots of twizzlers...|\n",
      "|    2|2012-09-19 17:00:00|          poor taste|\n",
      "|    5|2012-08-15 17:00:00|            Love it!|\n",
      "|    5|2011-12-22 16:00:00|  GREAT SWEET CANDY!|\n",
      "|    5|2011-10-07 17:00:00|Home delivered tw...|\n",
      "+-----+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = colSelect.transform(reviews_spark)\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create transformer which creates several time columns based on the `Time` colum\n",
    "\n",
    "The time is stored as a timestamp, however, we would like to get a more human readable information from it. We can use the SparkSQL data methods such as `month`, `dayofmonth` and so on to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "refineTime = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT  Score,\n",
    "            Summary, \n",
    "            dayofmonth(Time) as Day, \n",
    "            month(Time) as Month, year(Time) as Year, \n",
    "            weekofyear(Time) as WeekNum, \n",
    "            date_format(Time, 'EEE') as WeekDay, \n",
    "            hour(Time) as HourOfDay, \n",
    "            IF(date_format(Time, 'EEE')='Sat' OR date_format(Time, 'EEE')='Sun', 1, 0) as Weekend, \n",
    "            CASE \n",
    "                WHEN month(TIME)=12 OR month(Time)<=2 THEN 'Winter' \n",
    "                WHEN month(TIME)>=3 OR month(Time)<=5 THEN 'Spring' \n",
    "                WHEN month(TIME)>=6 AND month(Time)<=9 THEN 'Summer' \n",
    "                ELSE 'Autumn' END as Seasson \n",
    "    FROM __THIS__\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+-----+----+-------+-------+---------+-------+-------+\n",
      "|Score|             Summary|Day|Month|Year|WeekNum|WeekDay|HourOfDay|Weekend|Seasson|\n",
      "+-----+--------------------+---+-----+----+-------+-------+---------+-------+-------+\n",
      "|    5|Good Quality Dog ...| 26|    4|2011|     17|    Tue|       17|      0| Spring|\n",
      "|    1|   Not as Advertised|  6|    9|2012|     36|    Thu|       17|      0| Spring|\n",
      "|    4|\"Delight\" says it...| 17|    8|2008|     33|    Sun|       17|      1| Spring|\n",
      "|    2|      Cough Medicine| 12|    6|2011|     23|    Sun|       17|      1| Spring|\n",
      "|    5|         Great taffy| 20|   10|2012|     42|    Sat|       17|      1| Spring|\n",
      "|    4|          Nice Taffy| 11|    7|2012|     28|    Wed|       17|      0| Spring|\n",
      "|    5|Great!  Just as g...| 19|    6|2012|     25|    Tue|       17|      0| Spring|\n",
      "|    5|Wonderful, tasty ...|  2|    5|2012|     18|    Wed|       17|      0| Spring|\n",
      "|    5|          Yay Barley| 22|   11|2011|     47|    Tue|       16|      0| Spring|\n",
      "|    5|    Healthy Dog Food| 25|   10|2012|     43|    Thu|       17|      0| Spring|\n",
      "|    5|The Best Hot Sauc...|  7|    2|2005|      6|    Mon|       16|      0| Winter|\n",
      "|    5|My cats LOVE this...| 26|    8|2010|     34|    Thu|       17|      0| Spring|\n",
      "|    1|My Cats Are Not F...| 12|    6|2012|     24|    Tue|       17|      0| Spring|\n",
      "|    4|   fresh and greasy!|  4|   11|2010|     44|    Thu|       17|      0| Spring|\n",
      "|    5|Strawberry Twizzl...| 11|    3|2010|     10|    Thu|       16|      0| Spring|\n",
      "|    5|Lots of twizzlers...| 28|   12|2009|     53|    Mon|       16|      0| Winter|\n",
      "|    2|          poor taste| 19|    9|2012|     38|    Wed|       17|      0| Spring|\n",
      "|    5|            Love it!| 15|    8|2012|     33|    Wed|       17|      0| Spring|\n",
      "|    5|  GREAT SWEET CANDY!| 22|   12|2011|     51|    Thu|       16|      0| Winter|\n",
      "|    5|Home delivered tw...|  7|   10|2011|     40|    Fri|       17|      0| Spring|\n",
      "+-----+--------------------+---+-----+----+-------+-------+---------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined = refineTime.transform(selected)\n",
    "refined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove neutral reviews and classify the Scores\n",
    "\n",
    "We are not interested in the neutral reviews (reviews with the `Score=3`) as they would not add much information to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, IDF, CountVectorizer\n",
    "\n",
    "filterScore = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT  IF(Score<3,'NEGATIVE', 'POSITIVE') as Sentiment, Summary, Day, Month, Year,\n",
    "            WeekNum, WeekDay, HourOfDay, Weekend, Seasson \n",
    "    FROM __THIS__ WHERE Score !=3 \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+-------+\n",
      "|Sentiment|             Summary|Day|Month|Year|WeekNum|WeekDay|HourOfDay|Weekend|Seasson|\n",
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+-------+\n",
      "| POSITIVE|Good Quality Dog ...| 26|    4|2011|     17|    Tue|       17|      0| Spring|\n",
      "| NEGATIVE|   Not as Advertised|  6|    9|2012|     36|    Thu|       17|      0| Spring|\n",
      "| POSITIVE|\"Delight\" says it...| 17|    8|2008|     33|    Sun|       17|      1| Spring|\n",
      "| NEGATIVE|      Cough Medicine| 12|    6|2011|     23|    Sun|       17|      1| Spring|\n",
      "| POSITIVE|         Great taffy| 20|   10|2012|     42|    Sat|       17|      1| Spring|\n",
      "| POSITIVE|          Nice Taffy| 11|    7|2012|     28|    Wed|       17|      0| Spring|\n",
      "| POSITIVE|Great!  Just as g...| 19|    6|2012|     25|    Tue|       17|      0| Spring|\n",
      "| POSITIVE|Wonderful, tasty ...|  2|    5|2012|     18|    Wed|       17|      0| Spring|\n",
      "| POSITIVE|          Yay Barley| 22|   11|2011|     47|    Tue|       16|      0| Spring|\n",
      "| POSITIVE|    Healthy Dog Food| 25|   10|2012|     43|    Thu|       17|      0| Spring|\n",
      "| POSITIVE|The Best Hot Sauc...|  7|    2|2005|      6|    Mon|       16|      0| Winter|\n",
      "| POSITIVE|My cats LOVE this...| 26|    8|2010|     34|    Thu|       17|      0| Spring|\n",
      "| NEGATIVE|My Cats Are Not F...| 12|    6|2012|     24|    Tue|       17|      0| Spring|\n",
      "| POSITIVE|   fresh and greasy!|  4|   11|2010|     44|    Thu|       17|      0| Spring|\n",
      "| POSITIVE|Strawberry Twizzl...| 11|    3|2010|     10|    Thu|       16|      0| Spring|\n",
      "| POSITIVE|Lots of twizzlers...| 28|   12|2009|     53|    Mon|       16|      0| Winter|\n",
      "| NEGATIVE|          poor taste| 19|    9|2012|     38|    Wed|       17|      0| Spring|\n",
      "| POSITIVE|            Love it!| 15|    8|2012|     33|    Wed|       17|      0| Spring|\n",
      "| POSITIVE|  GREAT SWEET CANDY!| 22|   12|2011|     51|    Thu|       16|      0| Winter|\n",
      "| POSITIVE|Home delivered tw...|  7|   10|2011|     40|    Fri|       17|      0| Spring|\n",
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered = filterScore.transform(refined)\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the message\n",
    "\n",
    "Here we use the [RegexTokenizer](https://spark.apache.org/docs/2.1.0/ml-features.html#tokenizer) to tokenize the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"Summary\",\n",
    "                                outputCol=\"tokenized_summary\",\n",
    "                                pattern=\"[, ]\",\n",
    "                                toLowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+-------+--------------------+\n",
      "|Sentiment|             Summary|Day|Month|Year|WeekNum|WeekDay|HourOfDay|Weekend|Seasson|   tokenized_summary|\n",
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+-------+--------------------+\n",
      "| POSITIVE|Good Quality Dog ...| 26|    4|2011|     17|    Tue|       17|      0| Spring|[good, quality, d...|\n",
      "| NEGATIVE|   Not as Advertised|  6|    9|2012|     36|    Thu|       17|      0| Spring|[not, as, adverti...|\n",
      "| POSITIVE|\"Delight\" says it...| 17|    8|2008|     33|    Sun|       17|      1| Spring|[\"delight\", says,...|\n",
      "| NEGATIVE|      Cough Medicine| 12|    6|2011|     23|    Sun|       17|      1| Spring|   [cough, medicine]|\n",
      "| POSITIVE|         Great taffy| 20|   10|2012|     42|    Sat|       17|      1| Spring|      [great, taffy]|\n",
      "| POSITIVE|          Nice Taffy| 11|    7|2012|     28|    Wed|       17|      0| Spring|       [nice, taffy]|\n",
      "| POSITIVE|Great!  Just as g...| 19|    6|2012|     25|    Tue|       17|      0| Spring|[great!, just, as...|\n",
      "| POSITIVE|Wonderful, tasty ...|  2|    5|2012|     18|    Wed|       17|      0| Spring|[wonderful, tasty...|\n",
      "| POSITIVE|          Yay Barley| 22|   11|2011|     47|    Tue|       16|      0| Spring|       [yay, barley]|\n",
      "| POSITIVE|    Healthy Dog Food| 25|   10|2012|     43|    Thu|       17|      0| Spring|[healthy, dog, food]|\n",
      "| POSITIVE|The Best Hot Sauc...|  7|    2|2005|      6|    Mon|       16|      0| Winter|[the, best, hot, ...|\n",
      "| POSITIVE|My cats LOVE this...| 26|    8|2010|     34|    Thu|       17|      0| Spring|[my, cats, love, ...|\n",
      "| NEGATIVE|My Cats Are Not F...| 12|    6|2012|     24|    Tue|       17|      0| Spring|[my, cats, are, n...|\n",
      "| POSITIVE|   fresh and greasy!|  4|   11|2010|     44|    Thu|       17|      0| Spring|[fresh, and, grea...|\n",
      "| POSITIVE|Strawberry Twizzl...| 11|    3|2010|     10|    Thu|       16|      0| Spring|[strawberry, twiz...|\n",
      "| POSITIVE|Lots of twizzlers...| 28|   12|2009|     53|    Mon|       16|      0| Winter|[lots, of, twizzl...|\n",
      "| NEGATIVE|          poor taste| 19|    9|2012|     38|    Wed|       17|      0| Spring|       [poor, taste]|\n",
      "| POSITIVE|            Love it!| 15|    8|2012|     33|    Wed|       17|      0| Spring|         [love, it!]|\n",
      "| POSITIVE|  GREAT SWEET CANDY!| 22|   12|2011|     51|    Thu|       16|      0| Winter|[great, sweet, ca...|\n",
      "| POSITIVE|Home delivered tw...|  7|   10|2011|     40|    Fri|       17|      0| Spring|[home, delivered,...|\n",
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = regexTokenizer.transform(filtered)\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary words\n",
    "\n",
    "Some words do not bring much information for the resulting model. For this, we use the [StopWordsRemover](https://spark.apache.org/docs/2.1.0/ml-features.html#stopwordsremover) to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWordsRemover = StopWordsRemover(inputCol=regexTokenizer.getOutputCol(),\n",
    "                                    outputCol=\"CleanedSummary\",\n",
    "                                    caseSensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|Sentiment|             Summary|      CleanedSummary|\n",
      "+---------+--------------------+--------------------+\n",
      "| POSITIVE|Good Quality Dog ...|[good, quality, d...|\n",
      "| NEGATIVE|   Not as Advertised|        [advertised]|\n",
      "| POSITIVE|\"Delight\" says it...|   [\"delight\", says]|\n",
      "| NEGATIVE|      Cough Medicine|   [cough, medicine]|\n",
      "| POSITIVE|         Great taffy|      [great, taffy]|\n",
      "| POSITIVE|          Nice Taffy|       [nice, taffy]|\n",
      "| POSITIVE|Great!  Just as g...|[great!, good, ex...|\n",
      "| POSITIVE|Wonderful, tasty ...|[wonderful, tasty...|\n",
      "| POSITIVE|          Yay Barley|       [yay, barley]|\n",
      "| POSITIVE|    Healthy Dog Food|[healthy, dog, food]|\n",
      "| POSITIVE|The Best Hot Sauc...|[best, hot, sauce...|\n",
      "| POSITIVE|My cats LOVE this...|[cats, love, \"die...|\n",
      "| NEGATIVE|My Cats Are Not F...|[cats, fans, new,...|\n",
      "| POSITIVE|   fresh and greasy!|    [fresh, greasy!]|\n",
      "| POSITIVE|Strawberry Twizzl...|[strawberry, twiz...|\n",
      "| POSITIVE|Lots of twizzlers...|[lots, twizzlers,...|\n",
      "| NEGATIVE|          poor taste|       [poor, taste]|\n",
      "| POSITIVE|            Love it!|         [love, it!]|\n",
      "| POSITIVE|  GREAT SWEET CANDY!|[great, sweet, ca...|\n",
      "| POSITIVE|Home delivered tw...|[home, delivered,...|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopWordsRemoved = stopWordsRemover.transform(tokenized)\n",
    "stopWordsRemoved.select([\"Sentiment\", \"Summary\", \"CleanedSummary\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash the words\n",
    "\n",
    "The algorithms can efficiently work with the numeric values hence we create a numeric representation of them using [CountVectorizer](https://spark.apache.org/docs/2.1.0/ml-features.html#countvectorizer).\n",
    "\n",
    "CountVectorizer is very similar to [HashingTF](https://spark.apache.org/docs/2.1.0/ml-features.html#tf-idf) function, except that it preserves the mapping from the index back to the word using internal vocabulary.\n",
    "\n",
    "For example, if word `Dog` is stored in the hash at the index `100`, we can get the word back as `countVectorizerModel.vocabulary[100]`\n",
    "\n",
    "#### Trick #3: Set minDF parameter to limit number of words\n",
    "\n",
    "The minDF parameter ensures that only words which occur more the 100 times in our case are included. This also speeds the process of modelling and ensures that outliers does not affect our model that much.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer(inputCol=stopWordsRemover.getOutputCol(),\n",
    "                                  outputCol=\"frequencies\",\n",
    "                                  minDF=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick #4: Manually train the count vectorizer so we can see how it behaves before we execute the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVecModel = countVectorizer.fit(stopWordsRemoved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is 1528\n",
      "['great', 'good', 'best', 'love', 'coffee', 'tea', 'product', 'taste', 'delicious', 'excellent']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size is \" + str(len(countVecModel.vocabulary)))\n",
    "\n",
    "print(countVecModel.vocabulary[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|Sentiment|      CleanedSummary|         frequencies|\n",
      "+---------+--------------------+--------------------+\n",
      "| POSITIVE|[good, quality, d...|(1528,[1,10,12,35...|\n",
      "| NEGATIVE|        [advertised]|  (1528,[620],[1.0])|\n",
      "| POSITIVE|   [\"delight\", says]|  (1528,[402],[1.0])|\n",
      "| NEGATIVE|   [cough, medicine]|        (1528,[],[])|\n",
      "| POSITIVE|      [great, taffy]|(1528,[0,1430],[1...|\n",
      "| POSITIVE|       [nice, taffy]|(1528,[29,1430],[...|\n",
      "| POSITIVE|[great!, good, ex...|(1528,[1,59,126],...|\n",
      "| POSITIVE|[wonderful, tasty...|(1528,[15,37,1430...|\n",
      "| POSITIVE|       [yay, barley]|        (1528,[],[])|\n",
      "| POSITIVE|[healthy, dog, food]|(1528,[10,12,21],...|\n",
      "| POSITIVE|[best, hot, sauce...|(1528,[2,44,86,45...|\n",
      "| POSITIVE|[cats, love, \"die...|(1528,[3,12,23,41...|\n",
      "| NEGATIVE|[cats, fans, new,...|(1528,[12,41,79],...|\n",
      "| POSITIVE|    [fresh, greasy!]|   (1528,[83],[1.0])|\n",
      "| POSITIVE|[strawberry, twiz...|(1528,[13,19,665]...|\n",
      "| POSITIVE|[lots, twizzlers,...|  (1528,[407],[1.0])|\n",
      "| NEGATIVE|       [poor, taste]|(1528,[7,178],[1....|\n",
      "| POSITIVE|         [love, it!]|(1528,[3,32],[1.0...|\n",
      "| POSITIVE|[great, sweet, ca...|(1528,[0,43,924],...|\n",
      "| POSITIVE|[home, delivered,...|(1528,[352,1080],...|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorized = countVecModel.transform(stopWordsRemoved)\n",
    "vectorized.select([\"Sentiment\", \"CleanedSummary\", \"frequencies\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create inverse document frequencies model\n",
    "\n",
    "Here we use [tf-idf](https://spark.apache.org/docs/2.1.0/ml-features.html#tf-idf) method to reflect the importance of a term to a document in the given set of data. Please see the Spark documentation for more information at [tf-idf](https://spark.apache.org/docs/2.1.0/ml-features.html#tf-idf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=countVectorizer.getOutputCol(),\n",
    "          outputCol=\"tf_idf_frequencies\",\n",
    "          minDocFreq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually train the IDF model to see the results before we execute the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel = idf.fit(vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+\n",
      "|Sentiment|      CleanedSummary|         frequencies|  tf_idf_frequencies|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "| POSITIVE|[good, quality, d...|(1528,[1,10,12,35...|(1528,[1,10,12,35...|\n",
      "| NEGATIVE|        [advertised]|  (1528,[620],[1.0])|(1528,[620],[7.32...|\n",
      "| POSITIVE|   [\"delight\", says]|  (1528,[402],[1.0])|(1528,[402],[6.86...|\n",
      "| NEGATIVE|   [cough, medicine]|        (1528,[],[])|        (1528,[],[])|\n",
      "| POSITIVE|      [great, taffy]|(1528,[0,1430],[1...|(1528,[0,1430],[2...|\n",
      "| POSITIVE|       [nice, taffy]|(1528,[29,1430],[...|(1528,[29,1430],[...|\n",
      "| POSITIVE|[great!, good, ex...|(1528,[1,59,126],...|(1528,[1,59,126],...|\n",
      "| POSITIVE|[wonderful, tasty...|(1528,[15,37,1430...|(1528,[15,37,1430...|\n",
      "| POSITIVE|       [yay, barley]|        (1528,[],[])|        (1528,[],[])|\n",
      "| POSITIVE|[healthy, dog, food]|(1528,[10,12,21],...|(1528,[10,12,21],...|\n",
      "| POSITIVE|[best, hot, sauce...|(1528,[2,44,86,45...|(1528,[2,44,86,45...|\n",
      "| POSITIVE|[cats, love, \"die...|(1528,[3,12,23,41...|(1528,[3,12,23,41...|\n",
      "| NEGATIVE|[cats, fans, new,...|(1528,[12,41,79],...|(1528,[12,41,79],...|\n",
      "| POSITIVE|    [fresh, greasy!]|   (1528,[83],[1.0])|(1528,[83],[5.507...|\n",
      "| POSITIVE|[strawberry, twiz...|(1528,[13,19,665]...|(1528,[13,19,665]...|\n",
      "| POSITIVE|[lots, twizzlers,...|  (1528,[407],[1.0])|(1528,[407],[6.88...|\n",
      "| NEGATIVE|       [poor, taste]|(1528,[7,178],[1....|(1528,[7,178],[3....|\n",
      "| POSITIVE|         [love, it!]|(1528,[3,32],[1.0...|(1528,[3,32],[3.0...|\n",
      "| POSITIVE|[great, sweet, ca...|(1528,[0,43,924],...|(1528,[0,43,924],...|\n",
      "| POSITIVE|[home, delivered,...|(1528,[352,1080],...|(1528,[352,1080],...|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "afterIdf = idfModel.transform(vectorized)\n",
    "afterIdf.select([\"Sentiment\", \"CleanedSummary\", \"frequencies\", \"tf_idf_frequencies\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Remove Summary Column\n",
    "\n",
    "The algoritms do not understand the string values very well. This is also the reason why we transformed the data using TF-IDF and created a numeric values out of the `Summary` column. Now we can drop the original string information so we do not confuse the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeSummary = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT Sentiment, Day, Month, Year, WeekNum, WeekDay, HourOfDay, Weekend, Seasson, tf_idf_frequencies\n",
    "    FROM __THIS__ \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+----+-------+-------+---------+-------+-------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|WeekDay|HourOfDay|Weekend|Seasson|  tf_idf_frequencies|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+-------+--------------------+\n",
      "| POSITIVE| 26|    4|2011|     17|    Tue|       17|      0| Spring|(1528,[1,10,12,35...|\n",
      "| NEGATIVE|  6|    9|2012|     36|    Thu|       17|      0| Spring|(1528,[620],[7.32...|\n",
      "| POSITIVE| 17|    8|2008|     33|    Sun|       17|      1| Spring|(1528,[402],[6.86...|\n",
      "| NEGATIVE| 12|    6|2011|     23|    Sun|       17|      1| Spring|        (1528,[],[])|\n",
      "| POSITIVE| 20|   10|2012|     42|    Sat|       17|      1| Spring|(1528,[0,1430],[2...|\n",
      "| POSITIVE| 11|    7|2012|     28|    Wed|       17|      0| Spring|(1528,[29,1430],[...|\n",
      "| POSITIVE| 19|    6|2012|     25|    Tue|       17|      0| Spring|(1528,[1,59,126],...|\n",
      "| POSITIVE|  2|    5|2012|     18|    Wed|       17|      0| Spring|(1528,[15,37,1430...|\n",
      "| POSITIVE| 22|   11|2011|     47|    Tue|       16|      0| Spring|        (1528,[],[])|\n",
      "| POSITIVE| 25|   10|2012|     43|    Thu|       17|      0| Spring|(1528,[10,12,21],...|\n",
      "| POSITIVE|  7|    2|2005|      6|    Mon|       16|      0| Winter|(1528,[2,44,86,45...|\n",
      "| POSITIVE| 26|    8|2010|     34|    Thu|       17|      0| Spring|(1528,[3,12,23,41...|\n",
      "| NEGATIVE| 12|    6|2012|     24|    Tue|       17|      0| Spring|(1528,[12,41,79],...|\n",
      "| POSITIVE|  4|   11|2010|     44|    Thu|       17|      0| Spring|(1528,[83],[5.507...|\n",
      "| POSITIVE| 11|    3|2010|     10|    Thu|       16|      0| Spring|(1528,[13,19,665]...|\n",
      "| POSITIVE| 28|   12|2009|     53|    Mon|       16|      0| Winter|(1528,[407],[6.88...|\n",
      "| NEGATIVE| 19|    9|2012|     38|    Wed|       17|      0| Spring|(1528,[7,178],[3....|\n",
      "| POSITIVE| 15|    8|2012|     33|    Wed|       17|      0| Spring|(1528,[3,32],[3.0...|\n",
      "| POSITIVE| 22|   12|2011|     51|    Thu|       16|      0| Winter|(1528,[0,43,924],...|\n",
      "| POSITIVE|  7|   10|2011|     40|    Fri|       17|      0| Spring|(1528,[352,1080],...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "removedSummary = removeSummary.transform(afterIdf)\n",
    "removedSummary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create XGBoost model\n",
    "\n",
    "Here we are using H2O's estimator to train a H2O XGBoost model on `Sentiment` column with 50 trees (default). The full documentation for XGBoost is available at [H2O Documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysparkling.ml import ColumnPruner, H2OXGBoost\n",
    "\n",
    "xgboost = H2OXGBoost(ratio=0.8,\n",
    "             featuresCols=[idf.getOutputCol()],\n",
    "             predictionCol=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create the pipeline by defining all the stages\n",
    "\n",
    "Now we have all the pieces ready and can define the final pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[colSelect,\n",
    "                            refineTime,\n",
    "                            filterScore,\n",
    "                            regexTokenizer,\n",
    "                            stopWordsRemover,\n",
    "                            countVectorizer,\n",
    "                            idf,\n",
    "                            removeSummary,\n",
    "                            xgboost])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the pipeline model\n",
    "\n",
    "The `fit` call calls each trasformer and estimator in the pipeline and creates so called the `PipelineModel`. The model is trained from the cleaned data from previous transformers and the final model is ready to accept the raw data to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(reviews_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try predictions\n",
    "\n",
    "First, lets load data we use for the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "reviews_h2o_pred = h2o.upload_file(\"/Users/kuba/Downloads/AmazonReviews_Predictions.csv\", \"reviews_preds.hex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And convert them to Spark so we can run the Spark pipeline on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_spark_pred = hc.as_spark_frame(reviews_h2o_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+----+-------+-------+---------+-------+-------+--------------------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|WeekDay|HourOfDay|Weekend|Seasson|  tf_idf_frequencies|   prediction_output|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+-------+--------------------+--------------------+\n",
      "| POSITIVE|  7|    6|2012|     23|    Thu|       17|      0| Spring|(1528,[48,647,130...|[0.21149086952209...|\n",
      "| POSITIVE| 14|   12|2011|     50|    Wed|       16|      0| Winter|        (1528,[],[])|[0.21149086952209...|\n",
      "| POSITIVE| 13|    9|2011|     37|    Tue|       17|      0| Spring|(1528,[264,306],[...|[0.21149086952209...|\n",
      "| POSITIVE| 19|   10|2011|     42|    Wed|       17|      0| Spring|(1528,[26,452],[4...|[0.21149086952209...|\n",
      "| POSITIVE|  8|    9|2012|     36|    Sat|       17|      1| Spring|(1528,[36,1407],[...|[0.04857343435287...|\n",
      "| POSITIVE|  7|    2|2012|      6|    Tue|       16|      0| Winter|        (1528,[],[])|[0.21149086952209...|\n",
      "| NEGATIVE| 17|    6|2012|     24|    Sun|       17|      1| Spring|        (1528,[],[])|[0.21149086952209...|\n",
      "| POSITIVE| 26|   11|2010|     47|    Fri|       16|      0| Spring|(1528,[2,217,484,...|[0.02590548992156...|\n",
      "| POSITIVE|  3|    4|2011|     13|    Sun|       17|      1| Spring|(1528,[217],[6.27...|[0.21149086952209...|\n",
      "| POSITIVE| 18|    2|2012|      7|    Sat|       16|      1| Winter|(1528,[0,217,621]...|[0.03228282928466...|\n",
      "| POSITIVE| 31|    1|2012|      5|    Tue|       16|      0| Winter|(1528,[23,34,1244...|[0.18288576602935...|\n",
      "| POSITIVE| 28|    3|2009|     13|    Sat|       17|      1| Spring|(1528,[155,230],[...|[0.21149086952209...|\n",
      "| POSITIVE| 27|   12|2007|     52|    Thu|       16|      0| Winter|(1528,[40],[4.812...|[0.04527455568313...|\n",
      "| POSITIVE| 12|    6|2012|     24|    Tue|       17|      0| Spring|(1528,[610],[7.33...|[0.21149086952209...|\n",
      "| NEGATIVE| 27|    4|2012|     17|    Fri|       17|      0| Spring|(1528,[6,973],[3....|[0.21149086952209...|\n",
      "| POSITIVE| 28|    1|2010|      4|    Thu|       16|      0| Winter|(1528,[213,305,13...|[0.21149086952209...|\n",
      "| POSITIVE|  6|    1|2010|      1|    Wed|       16|      0| Winter|(1528,[36,305],[4...|[0.04857343435287...|\n",
      "| POSITIVE| 29|    3|2010|     13|    Mon|       17|      0| Spring|(1528,[61],[5.269...|[0.21149086952209...|\n",
      "| POSITIVE| 23|    6|2010|     25|    Wed|       17|      0| Spring|(1528,[1268],[8.1...|[0.21149086952209...|\n",
      "| POSITIVE| 12|    9|2009|     37|    Sat|       17|      1| Spring|(1528,[1],[2.5776...|[0.09792464971542...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.transform(reviews_spark_pred).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the pipeline model\n",
    "\n",
    "Later we use the pipeline model in the Scala to demonstrate the deployment of the pipeline in the JVM world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"reviews_pipeline.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick #5: Check variable inportances\n",
    "\n",
    "We can inspect the model in Flow and see the variable importances. However we do not have information about the words, just the indices. We can ask the `CountVectorizer` what word is on the specific index to see what words affect our model the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[5].vocabulary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Deply the Application\n",
    "\n",
    "Right now, we defined the PySPark pipeline. We will now demonstrate its deployment using Spark Streaming application in Scala where the pipeline defined above will receive raw streaming data and run preditions on them right away.\n",
    "\n",
    "The steps will be:\n",
    "\n",
    " - Load the schema from the schema file\n",
    " - Create input data stream and pass it the schema. The input data stream will point to a directory where a new csv files will be comming from different streaming sources\n",
    " - Load the pipeline from the pipeline file\n",
    " - Create output data stream. For our purposes, we store the data into memory and also to a SparkSQL table\n",
    " - We can inspect the predictions  by regularly displaying the content of the desired table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jakubs-mbp.0xdata.loc:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10fb3d940>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check again we have spark available\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o58.load.\n: java.lang.ClassNotFoundException: py_sparkling.ml.models.H2OMOJOModel\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:238)\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadParamsInstance(ReadWrite.scala:651)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$4.apply(Pipeline.scala:274)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$4.apply(Pipeline.scala:272)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:272)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:348)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:342)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-7895329fbbe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the exported pipeline model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipelineModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpipeline_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reviews_pipeline.model/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/devel/programs/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/devel/programs/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaultParamsReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'language'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paramMap'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paramMap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'language'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mJavaMLReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineSharedReadWrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/devel/programs/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clazz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_from_java\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             raise NotImplementedError(\"This Java ML type cannot be loaded into Python currently: %r\"\n",
      "\u001b[0;32m~/devel/programs/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/devel/programs/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/devel/programs/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o58.load.\n: java.lang.ClassNotFoundException: py_sparkling.ml.models.H2OMOJOModel\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:238)\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadParamsInstance(ReadWrite.scala:651)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$4.apply(Pipeline.scala:274)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$4.apply(Pipeline.scala:272)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:272)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:348)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:342)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# Load the exported pipeline model\n",
    "from pyspark.ml import PipelineModel\n",
    "pipeline_model = PipelineModel.load(\"reviews_pipeline.model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Id,IntegerType,false),StructField(ProductId,StringType,false),StructField(UserId,StringType,false),StructField(ProfileName,StringType,false),StructField(HelpfulnessNumerator,ShortType,false),StructField(HelpfulnessDenominator,ShortType,false),StructField(Score,ByteType,false),StructField(Time,IntegerType,false),StructField(Summary,StringType,false),StructField(Text,StringType,false)))\n"
     ]
    }
   ],
   "source": [
    "# Load exported schema of input data\n",
    "from pyspark.sql.types import StructType\n",
    "import json\n",
    "\n",
    "schema = StructType.fromJson(json.load(open(\"schema.json\", 'r')))\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x116342f28>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import Popen\n",
    "Popen([\"./start_streaming.sh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.csv  15.csv 21.csv 28.csv 34.csv 40.csv 47.csv 53.csv 6.csv  66.csv\r\n",
      "1.csv  16.csv 22.csv 29.csv 35.csv 41.csv 48.csv 54.csv 60.csv 67.csv\r\n",
      "10.csv 17.csv 23.csv 3.csv  36.csv 42.csv 49.csv 55.csv 61.csv 7.csv\r\n",
      "11.csv 18.csv 24.csv 30.csv 37.csv 43.csv 5.csv  56.csv 62.csv 8.csv\r\n",
      "12.csv 19.csv 25.csv 31.csv 38.csv 44.csv 50.csv 57.csv 63.csv 9.csv\r\n",
      "13.csv 2.csv  26.csv 32.csv 39.csv 45.csv 51.csv 58.csv 64.csv\r\n",
      "14.csv 20.csv 27.csv 33.csv 4.csv  46.csv 52.csv 59.csv 65.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_stream = spark.readStream.schema(schema).csv(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c91833045132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_data_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline_model' is not defined"
     ]
    }
   ],
   "source": [
    "output_data_stream = pipeline_model.transform(input_data_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps above are written using Scala as:\n",
    "```\n",
    " val spark = SparkSession.builder().master(\"local\").getOrCreate()\n",
    "\n",
    "      //\n",
    "      // Load exported pipeline\n",
    "      //\n",
    "      import org.apache.spark.sql.types.DataType\n",
    "      val pipelineModel = PipelineModel.read.load(\"py/examples/pipelines/reviews_pipeline.model/\")\n",
    "\n",
    "      //\n",
    "      // Load exported schema of input data\n",
    "      //\n",
    "      val schema = StructType(DataType.fromJson(scala.io.Source.fromFile(\"py/examples/pipelines/schema.json\").mkString).asInstanceOf[StructType].map {\n",
    "        case StructField(name, dtype, nullable, metadata) => StructField(name, dtype, true, metadata)\n",
    "        case rec => rec\n",
    "      })\n",
    "      println(schema)\n",
    "\n",
    "      //\n",
    "      // Define input stream\n",
    "      //\n",
    "      val inputDataStream = spark.readStream.schema(schema).csv(\"py/examples/data/kuba/input/*.csv\")\n",
    "\n",
    "      //\n",
    "      // Apply loaded model\n",
    "      //\n",
    "      val outputDataStream = pipelineModel.transform(inputDataStream)\n",
    "\n",
    "      //\n",
    "      // Forward output stream into memory-sink\n",
    "      //\n",
    "      outputDataStream.writeStream.format(\"memory\").queryName(\"predictions\").start()\n",
    "\n",
    "      //\n",
    "      // Query results\n",
    "      //\n",
    "      while(true){\n",
    "        spark.sql(\"select * from predictions\").show()\n",
    "        Thread.sleep(5000)\n",
    "      }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see it in practice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
